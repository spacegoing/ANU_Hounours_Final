% Preamble
% ---
\documentclass{article}

% Packages
% ---
\usepackage{amsmath} % Advanced math typesetting
\usepackage{amsfonts} % Advanced math typesetting
\usepackage{amssymb} % More Math
\usepackage[utf8]{inputenc} % Unicode support (Umlauts etc.)
\usepackage[ngerman]{babel} % Change hyphenation rules
\usepackage{hyperref} % Add a link to your document
\usepackage{graphicx} % Add pictures to your document
\usepackage{listings} % Source code formatting and highlighting

\DeclareMathOperator*{\argmax}{arg\,max}


% Title
%---
\title{Optimizing Weighted Lower Linear Envelope Potentials Within Latent-SVM Framework}
\author{Chang Li}
\date{\today}

% Document
%---
\begin{document}
	\maketitle
	
	\section{Introduction}
	One interesting task in machine learning is labeling over complex and structured objects. Many applications such as image segmentation, motif finding and noun-phrase parsing involved with representing jointly correlated variables. Encoding consistency constraints over large number of cliques, for example, is central to the problem of image segmentation. Algorithm frameworks like Markov Random Field (MRF) containing higher order energy functions and max margin method for Structural SVM are raising interests recently due to their capability of representing structural dependencies of variables and ensuring computationally feasible approximate. \\
	Lower linear envelope potentials is one of higher order energy functions defined on MRF which becomes popular due to their ability to encode consistency relationship between labels in clique. Gould\cite{gouldlearning} investigated the submodularity of lower linear envelope potentials and developed a graph-cuts algorithm to perform exact inference on them. Then they proposed a Max-Margin framework to optimize potentials' parameters. However, in order to write the energy function into a linear combination, they sampled the lower linear envelope potentials using a set of fixed space points. Althought this formulation can be globally optimized by using the Max-Margin framework, it lost a rich class of representations of energy function due to the fixed space sampling. Removing the equally spaced constraint and introduce their auxiliary variables back will result in a latent SVM formulation.\\
	In practical, many information providing useful cues for prediction is not directly observable from data. For motif (repeated patterns in DNA sequences) finding problem, as an example, the task if to find motifs from a set of DNA sequences where the location of these motifs are unknown. Thus the information of position can be treated as hidden variable and is important to be considered in the model though it is not directly observable. Issues like this have been well studied by many researchers and latent SVMs, which can explicitly model hidden variables with joint feature vectors, outperforms many other methods. \\
	The latent SVM was developed by Felzenszwalb et al.\cite{felzenszwalb2008discriminatively} and Yu and Joachims\cite{yu2009learning} independently in different ways. The main idea is introducing a latent variable to extend the feature vector, which results in an arbitrary loss function, e.g. Hinge Loss, with an upper bound. Then the optimization was done by using Concave-Convex Procedure (CCCP) algorithm.\\
	In this project, our goal is to extend previous research\cite{gouldlearning} and optimize the weighted lower linear envelope potentials using latent SVM framework. We introduced the auxiliary variables back and rewrite it into a linear combination of weights and features. We will first experiment our algorithm on synthetic data. 

	\subsection{Literature Review}
	Learning structural objects from unknown probability distribution is becoming popular in recent years. Tsochantaridis et al.\cite{tsochantaridis2005large} generalized multiclass SVMs\cite{crammer2002algorithmic} to structural SVMs by extending feature vectors to joint feature vectors which map features extracted jointly over input-output pairs to discrete output. The exact maximum a posteriori (MAP) problem thus becomes an NP-hard problem. They overcome this by generalize the hard margin into "soft" margin and found an upper bound of arbitrary loss functions under this formulation.\\
	Based on the previous research, Yu and Joachims\cite{yu2009learning} developed latent SVM by introducing a hidden variable into the joint feature vector. They observed a fact that in real world applications hidden variables are usually intermediate results and are not required as an output. With this fact they followed “Soft-Margin” method and found an upper bound for the loss function with latent variables. However, the resulted object function is still non-convex.\\
	Yuille and Rangarajan \cite{yuille2002concave} developed the Concave-Convex Procedure (CCCP) which is guaranteed to find a local minimum for a Difference-Convex (DC) program. Yu and Joachims\cite{yu2009learning} combined CCCP algorithm by writing their non-convex object function into a difference of two convex functions and came up with an EM like 2 steps optimizing algorithm. For each iteration, they first compute latent variables utilizing current parameter vectors and then in turn optimizing parameter vectors using the standard Structural SVM algorithm with previously computed latent variables. \\
	Higher order potentials are raising interests due to their capability to represent dependencies between complex objects. Kohli and Kumar\cite{kohli2009robust} proposed a method to represent a class of higher order potentials with lower (upper) linear envelope potentials. By introducing auxiliary variables, they reduced the linear representation to a pairwise form and proposed an approximate algorithm with standard linear programming methods. Following their routine, Gould\cite{gouldlearning} extended their method to a weighted lower linear envelope in binary Markov Random Fields (MRF) which can be solved with an efficient algorithm. They showed the energy function with auxiliary variables is submodular by transforming it into a quadratic pseudo-Boolean form and how “graph-cuts” like algorithm can be applied to do exact inference. They then optimized the model’s parameters under the max margin framework\cite{tsochantaridis2005large}. \\
	In their work they pointed out the potential relationship between their auxiliary representation and latent SVM\cite{yu2009learning}. However, since removing of their fixed space constraint will result dependence between latent variable and parameters, further research still remains open.\\
	This report will study the latent variable formulation of their energy function and optimize it using latent SVM framework. The rest of report is organized as follows: Section 2 gives a briefly introduction of Latent Structural SVM. Section 3 presents the formulation of weighted lower linear envelope potential and its most important properties. Section 4 describes the linear combination formulation of the energy function and how to optimize it using latent structural SVM. Section 5 discusses its performance on synthetic data.
	

	\section{Latent Structural SVMs}
		To include unobserved information in the model, Yu\cite{yu2009learning} extended the joint feature function\cite{tsochantaridis2005large}  $\Psi(\mathbf{x},\mathbf{y}) $ with a latent variable $\mathbf{h}\in \mathcal{H}$ to $\Psi(\mathbf{x},\mathbf{y},\mathbf{h}) $. So the inference problem becomes
		$$
		f_w(x) = \argmax_{(\mathbf{y} \times \mathbf{h}) \in \mathcal{Y} \times \mathcal{H}} w\cdot\Psi(\mathbf{x},\mathbf{y},\mathbf{h})
		$$
		Accordingly, the loss function can be extended as
		$$
		\Delta((\mathbf{y}_i,\mathbf{h}^*_i(w)),(\mathbf{\hat{y}}_i(w),\mathbf{\hat{h}}_i(w)))
		$$
		where
		$$
		\mathbf{h}^*_i(w) = \argmax_{\mathbf{h} \in \mathcal{H}} w \cdot \Psi(\mathbf{x}_i,\mathbf{y}_i,\mathbf{h})
		$$
		$$
		(\mathbf{\hat{y}}_i(w),\mathbf{\hat{h}}_i(w))=\argmax_{(\mathbf{y} \times \mathbf{h}) \in \mathcal{Y} \times \mathcal{H}} w\cdot\Psi(\mathbf{x}_i,\mathbf{y},\mathbf{h})
		$$
		However, under this formulation the ``loss augmented inference'' used in Structral SVMs\cite{tsochantaridis2005large} to remove the complexity cannot be performed due to the dependence of loss function $\Delta$ on hidden variables $\mathbf{h}^*_i(w)$. Yu argued that in real world applications hidden variables are usually intermediate results and are not required as an output\cite{yu2009learning}. Therefore, the loss function can only focus on the inferenced hidden variables $\mathbf{\hat{h}}_i(w)$ which leads to:
		$$
		\Delta((\mathbf{y}_i,\mathbf{h}^*_i(w)),(\mathbf{\hat{y}}_i(w),\mathbf{\hat{h}}_i(w))) = \Delta(\mathbf{y}_i,\mathbf{\hat{y}}_i(w),\mathbf{\hat{h}}_i(w))
		$$
		Thus the upper bound used in structural SVMs can be written as:
		\begin{align*}
			\Delta((\mathbf{y}_i,\mathbf{h}^*_i(w)),(\mathbf{\hat{y}}_i(w),\mathbf{\hat{h}}_i(w)))
			&\leq \bigg(\max_{(\mathbf{\hat{y}} \times \mathbf{\hat{h}}) \in \mathcal{Y} \times \mathcal{H}} [w\cdot\Psi(\mathbf{x}_i,\mathbf{\hat{y}},\mathbf{\hat{h}}) + \Delta(\mathbf{y}_i,\mathbf{\hat{y}},\mathbf{\hat{h}})]\bigg)\\
			&-\max_{\mathbf{h} \in \mathcal{H}} w \cdot \Psi(\mathbf{x}_i,\mathbf{y}_i,\mathbf{h})
		\end{align*}
		Hence the optimization problem for Structural SVMs with latent variables becomes
		\begin{align*}
			\min_w\bigg(\frac{1}{2}\|w\|^2+
			C\sum_{i=1}^{n}\big(\max_{(\mathbf{\hat{y}} \times \mathbf{\hat{h}}) \in \mathcal{Y} \times \mathcal{H}} [w\cdot\Psi(\mathbf{x}_i,\mathbf{\hat{y}},\mathbf{\hat{h}}) + \Delta(\mathbf{y}_i,\mathbf{\hat{y}},\mathbf{\hat{h}})]\big)\bigg)\\
			-C\sum_{i=1}^{n}\big(\max_{\mathbf{h} \in \mathcal{H}} w \cdot \Psi(\mathbf{x}_i,\mathbf{y}_i,\mathbf{h})\big)
		\end{align*}
		Observing that the former equation is a difference of two convex functions, therefore the redefined objective function can be optimized locally using CCCP algorithm.
		

		\section{Weighted Lower Linear Envelope Potentials}
		Suppose we have a binary MRFs $\mathbf{y}=\{y_1,\dots,y_n\}$, $y_i\in\{0,1\}$. A higher-order potential $\psi_c^H(\mathbf{y}_c)$ is an arbitrary function defined on cliques $\mathbf{y}_c=\{y_i : i\in c\}$ where $c\subseteq\{1,\dots,n\}$. Gould\cite{gouldlearning} proposed it with a weighted lower linear envelope potential expression
		\begin{align}
		\psi_c^H(\mathbf{y}_c) \triangleq \min_{k=1,\dots,K}\bigg\{a_kW_c(\mathbf{y}_c)+b_k\bigg\}
		\end{align}
		where $(a_k,b_k)\in\mathbb{R}^2$ are linear function parameters and
		$$
		W_c(\mathbf{y}_c) = \sum_{i\in c}^{}w_i^cy_i
		$$
		where c is a clique. $w_i^c$ is a per-variable non-negative weights for every nodes in each clique and satisfies $\sum_ {i\in c}^{}w_i^c=1$.\\
		In the literature they explored many important properties of lower linear envelope. They proved that by maitaining the order of variables $a_k$ and $b_k$, the encoding is ensured free of redundant linear functions (Proposition 3.1\cite{gouldlearning}):
		\begin{align}
		a_k > a_{k+1}\\
		b_k < b_{k+1}
		\end{align}
		Another important property is that the inferenced assignment $y^*$ is irrelevant of arbitrarily shifting the piece-wise functions set up or down. Let $ \tilde{\psi}_c^H(y_c) = \min_{k=1,\dots,K}{a_kW_c(y_c)+b_k+b^{const}}$. We can get:
		$$
		arg\min_{y_c}{\psi_c^H(y_c)}=arg\min_{y_c}{\tilde{\psi}_c^H(y_c)}
		$$
		Therefore, we can shift arbitrary envelope to zero:
		\begin{align}
		b_1 = 0
		\end{align}
		From equation (3) we know that variable $b_k$ maintaining increasing order, thus $b_k>0$ when $k>1$.\\
		To help with exact inference on the envelope, Gould\cite{gouldlearning} rewrite it into a quadratic pseudo-Boolean function by introducing $K-1$ auxiliary binary variables $\mathbf{z} = {z_1,\dots,z_K-1}$:
		\begin{align}
		E^c(y_c,z)=a_1W_c(y_c)+b_1+\sum_{k=1}^{K-1}z_k((a_{k+1}-a_k)W_c(y_c)+b_{k+1}-b_k)
		\end{align}
		It is worth to notice that as long as we maintain the linear constraints on variables $a_k$ and $b_k$, the following constraint is automatically satisfied (Observation 3.6\cite{gouldlearning}):
		\begin{align}
		z_{k+1}\le z_k
		\end{align}
		In the literature they also proved the submodularity of equation (5) and proposed a graph-cuts method to perform the exact inference on it. Our work is based on these results.
		
		\section{Optimizing Lower Linear Envelope Using Latent Structural SVMs}
		To optimize the energy function using latent structral SVMs, it first requires us write the equation (5) into a linear combination of weights and feature vector. In order to help with the formulation, we first define $k^* = \argmax_{k=1,\dots,K-1} \big[\big[z_{k}\big]\big]$. We also notice that the equation (5) can be rewrite as an inner product of two vectors ($b_1=0$ according to equation  (4)):
		\begin{align*}
		E^c(y_c,z)&=a_1W_c(y_c)+b_1+\sum_{k=1}^{K-1}z_k((a_{k+1}-a_k)W_c(y_c)+b_{k+1}-b_k)\\
		&=a_1W_c(y_c)+\sum_{k=1}^{K-1}(a_{k+1}-a_k)z_kW_c(y_c)+\sum_{k=1}^{K-1}(b_{k+1}-b_k)z_k
		\end{align*}
		One thing to notice is that $z_{k^*+1}=0$. From equation (6) we know that $z_k = 1$ from $k=1$ to $k=k^*$ and all variables $z_k$ after $k^*$ are $0$s. We can reparameterized the energy function as $E^c(y_c,z)=\mathbf{w}\mathbf{\phi}$ where:
		\begin{equation}
			w_i = \left\{
			\begin{aligned}
				& a_1	& \text{for} \ i=1\\
				& a_i-a_{i-1} & \text{for}\ 1< i \leq K\\
				& b_{i+1-K}-b_{i-K} & \text{for} \ K<i\le2K-1\\
			\end{aligned}
			\right.
		\end{equation}
		
		\begin{equation}
		\phi_i = \left\{
		\begin{aligned}
		& W(\mathbf{y}) 	& \text{for} \ i=1\\
		& W(\mathbf{y})\bigg[\bigg[i-1\le k^*\bigg]\bigg] & \text{for}\ 1<i\le K\\
		& \bigg[\bigg[ i-K\le k^*\bigg]\bigg]  & \text{for} \ K<i\le2K-1\\
		\end{aligned}
		\right.
		\end{equation}
		Thus our inference problem becomes:
		$$
		(\mathbf{\hat{y}}_i(w),\mathbf{\hat{z}}_i(w))=\argmax_{(\mathbf{y} \times \mathbf{z}) \in \mathcal{Y} \times \mathcal{Z}} w\cdot\Psi(\mathbf{y},\mathbf{z})
		$$
		$$
		\mathbf{z}^*_i(w) = \argmax_{\mathbf{z} \in \mathcal{Z}} w \cdot \Psi(\mathbf{y}_i,\mathbf{z})
		$$
		We can perform the graph-cuts algorithm proposed by Gould\cite{gouldlearning} on both equations. For the later one we simply use the ground-truth labels $y_i$ as function input. Together with linear constraints (2) and (3), we can optimize the energy function using latent SVMs.

	\renewcommand\refname{Bibliography}
	\bibliographystyle{ieeetr}
	\bibliography{draft}
\end{document}